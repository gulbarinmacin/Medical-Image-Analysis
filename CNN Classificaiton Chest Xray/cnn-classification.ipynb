{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qT2H8RLY0Vx8",
        "outputId": "0dca9bae-d4fe-448c-e404-2eef87f7e04b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import concurrent.futures\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tensorflow.keras.metrics import Precision, Recall\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [ 'NORMAL', 'PNEUMONIA']\n",
        "img_size = 120\n",
        "\n",
        "# Function to preprocess a single image\n",
        "def process_image(image_path, class_num):\n",
        "    try:\n",
        "        img_arr = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "        if img_arr is None:\n",
        "            raise ValueError(f\"Image not loaded properly: {image_path}\")\n",
        "        resized_arr = cv2.resize(img_arr, (img_size, img_size))\n",
        "        return resized_arr, class_num\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return None"
      ],
      "metadata": {
        "id": "6NkOInDzu0Hy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load and preprocess the data using parallel processing\n",
        "def get_data(data_dir):\n",
        "    data = []\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        futures = []\n",
        "        for c in classes:\n",
        "            path = os.path.join(data_dir, c)\n",
        "            class_num = classes.index(c)\n",
        "            for img in os.listdir(path):\n",
        "                futures.append(executor.submit(process_image, os.path.join(path, img), class_num))\n",
        "\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            result = future.result()\n",
        "            if result:\n",
        "                data.append(result)\n",
        "    return data"
      ],
      "metadata": {
        "id": "A79hog6fu3vN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/drive/My Drive/chest_xray'\n",
        "train_data = get_data(os.path.join(data_dir, 'train'))\n",
        "test_data = get_data(os.path.join(data_dir, 'test'))\n",
        "val_data = get_data(os.path.join(data_dir, 'val'))"
      ],
      "metadata": {
        "id": "jtOlu1-UJ4d6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features and labels\n",
        "x_train, y_train = zip(*train_data)\n",
        "x_test, y_test = zip(*test_data)\n",
        "x_val, y_val = zip(*val_data)\n",
        "\n",
        "# Convert data to numpy arrays and normalize\n",
        "x_train = np.array(x_train) / 255.0\n",
        "x_test = np.array(x_test) / 255.0\n",
        "x_val = np.array(x_val) / 255.0\n",
        "\n",
        "# Convert labels to numpy arrays\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "y_val = np.array(y_val)\n",
        "\n",
        "# Reshape data for deep learning\n",
        "x_train = x_train.reshape(-1, img_size, img_size, 1)\n",
        "x_test = x_test.reshape(-1, img_size, img_size, 1)\n",
        "x_val = x_val.reshape(-1, img_size, img_size, 1)"
      ],
      "metadata": {
        "id": "2UtHjdyxVa-e"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define data augmentation generator\n",
        "train_datagen = ImageDataGenerator(\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "train_datagen.fit(x_train)"
      ],
      "metadata": {
        "id": "hQC-yYvxr4bS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute class weights\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights = dict(enumerate(class_weights))"
      ],
      "metadata": {
        "id": "2hw4EkSx9Jrp"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the enhanced CNN model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(img_size, img_size, 1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "Ludrz2BcmTAM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "mbKo_Y_mmV8U"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "lB26kdNZ-x2E"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_datagen.flow(x_train, y_train, batch_size=64),\n",
        "    steps_per_epoch=len(x_train) / 64,\n",
        "    epochs=5,\n",
        "    validation_data=(x_val, y_val),\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[early_stopping, reduce_lr]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlWCNSQzV574",
        "outputId": "d1d42eef-0832-4488-cf9a-b86265514687"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "21/21 [==============================] - 103s 5s/step - loss: 0.1777 - accuracy: 0.9186 - val_loss: 7.1066 - val_accuracy: 0.7103 - lr: 0.0010\n",
            "Epoch 2/5\n",
            "21/21 [==============================] - 102s 5s/step - loss: 0.1813 - accuracy: 0.9186 - val_loss: 6.2272 - val_accuracy: 0.7103 - lr: 0.0010\n",
            "Epoch 3/5\n",
            "21/21 [==============================] - 144s 7s/step - loss: 0.1932 - accuracy: 0.9121 - val_loss: 3.4748 - val_accuracy: 0.7207 - lr: 0.0010\n",
            "Epoch 4/5\n",
            "21/21 [==============================] - 141s 7s/step - loss: 0.1708 - accuracy: 0.9331 - val_loss: 0.9290 - val_accuracy: 0.8408 - lr: 0.0010\n",
            "Epoch 5/5\n",
            "21/21 [==============================] - 102s 5s/step - loss: 0.1556 - accuracy: 0.9339 - val_loss: 0.4320 - val_accuracy: 0.8789 - lr: 0.0010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loss: \" , model.evaluate(x_test,y_test)[0])\n",
        "print(\"Accuracy:\" , model.evaluate(x_test,y_test)[1]*100 , \"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7NC5YGMk8Pz",
        "outputId": "dc2488d7-8072-440d-a58b-647f9e1b3913"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20/20 [==============================] - 7s 325ms/step - loss: 0.6416 - accuracy: 0.8429\n",
            "Loss:  0.6416131854057312\n",
            "20/20 [==============================] - 7s 332ms/step - loss: 0.6416 - accuracy: 0.8429\n",
            "Accuracy: 84.29487347602844 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oFIwX-O_lPQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DaSOX_zMkQcG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}